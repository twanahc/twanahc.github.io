---
layout: post
title: "AI Color Grading: Achieving Visual Consistency Across Generated Clips with Style Transfer and LUT Generation"
date: 2026-01-05
category: architecture
---

If you have ever stitched together three AI-generated clips and watched the result, you know the problem immediately. Clip one looks warm and golden. Clip two is cool and desaturated. Clip three has crushed blacks and a teal-orange split tone that came from nowhere. Each clip is individually fine. Together they look like a ransom note assembled from three different movies.

This is the color consistency problem, and it is one of the most visible quality gaps between AI-generated multi-shot video and professional footage. Professional colorists spend hours ensuring every shot in a film shares a unified visual language. AI video generators have no concept of this --- each clip is generated independently, with its own implicit color grade baked into the pixels by the diffusion process.

This post is the complete technical treatment of solving color consistency in AI video pipelines. We cover color space theory, histogram matching algorithms, neural style transfer for color, 3D LUT generation, FFmpeg filter chains, perceptual color difference metrics, and a full automated pipeline that takes N inconsistent clips and produces N color-consistent clips. Every formula is derived. Every algorithm has code.

---

## Table of Contents

1. [The Problem in Detail](#the-problem-in-detail)
2. [Color Space Fundamentals](#color-space-fundamentals)
3. [Color Histogram Matching](#color-histogram-matching)
4. [Neural Style Transfer for Color Grading](#neural-style-transfer-for-color-grading)
5. [3D LUT Generation](#3d-lut-generation)
6. [FFmpeg Color Grading Toolkit](#ffmpeg-color-grading-toolkit)
7. [Automated Color Consistency Pipeline](#automated-color-consistency-pipeline)
8. [Perceptual Color Difference: CIEDE2000](#perceptual-color-difference-ciede2000)
9. [Putting It All Together](#putting-it-all-together)

---

## The Problem in Detail {#the-problem-in-detail}

When a diffusion model generates a video clip, the color characteristics of the output are determined by a combination of factors: the text prompt, the noise seed, the model's training data distribution, and the internal dynamics of the denoising process. None of these factors are explicitly controlled for color consistency across separate generation calls.

Consider a typical multi-shot project: a 30-second product video composed of five 6-second clips generated by Veo 3.1. Even with identical style instructions in every prompt, you get:

- **Different white balance** across clips (some warm, some cool)
- **Different contrast curves** (some flat, some punchy)
- **Different saturation levels** (some vivid, some muted)
- **Different black and white points** (some lifted blacks, some crushed)
- **Different color casts** (slight green tint in one, magenta in another)

The root cause is that diffusion models learn to generate plausible images, not consistent images. The training objective $L = \mathbb{E}\left[\|\epsilon - \epsilon_\theta(x_t, t, c)\|^2\right]$ optimizes for individual sample quality. There is no cross-sample consistency term.

<svg viewBox="0 0 800 300" xmlns="http://www.w3.org/2000/svg" style="max-width:800px; margin: 2em auto; display: block;">
  <rect width="800" height="300" fill="white"/>
  <!-- Title -->
  <text x="400" y="30" font-family="Georgia, serif" font-size="16" fill="#333" text-anchor="middle" font-weight="bold">The Color Consistency Problem: Five Clips, Five Color Grades</text>

  <!-- Clip boxes with different "color grades" -->
  <rect x="30" y="60" width="130" height="80" rx="6" fill="#e8d5b7" stroke="#333" stroke-width="1.5"/>
  <text x="95" y="105" font-family="monospace" font-size="11" fill="#333" text-anchor="middle">Clip 1: Warm</text>
  <text x="95" y="120" font-family="monospace" font-size="10" fill="#666" text-anchor="middle">WB: 6500K</text>

  <rect x="180" y="60" width="130" height="80" rx="6" fill="#b7d5e8" stroke="#333" stroke-width="1.5"/>
  <text x="245" y="105" font-family="monospace" font-size="11" fill="#333" text-anchor="middle">Clip 2: Cool</text>
  <text x="245" y="120" font-family="monospace" font-size="10" fill="#666" text-anchor="middle">WB: 4200K</text>

  <rect x="330" y="60" width="130" height="80" rx="6" fill="#2a3a3a" stroke="#333" stroke-width="1.5"/>
  <text x="395" y="105" font-family="monospace" font-size="11" fill="#ddd" text-anchor="middle">Clip 3: Crushed</text>
  <text x="395" y="120" font-family="monospace" font-size="10" fill="#aaa" text-anchor="middle">High contrast</text>

  <rect x="480" y="60" width="130" height="80" rx="6" fill="#d5e8d5" stroke="#333" stroke-width="1.5"/>
  <text x="545" y="105" font-family="monospace" font-size="11" fill="#333" text-anchor="middle">Clip 4: Green</text>
  <text x="545" y="120" font-family="monospace" font-size="10" fill="#666" text-anchor="middle">Color cast</text>

  <rect x="630" y="60" width="130" height="80" rx="6" fill="#e8b7d5" stroke="#333" stroke-width="1.5"/>
  <text x="695" y="105" font-family="monospace" font-size="11" fill="#333" text-anchor="middle">Clip 5: Magenta</text>
  <text x="695" y="120" font-family="monospace" font-size="10" fill="#666" text-anchor="middle">Color cast</text>

  <!-- Arrow down -->
  <line x1="400" y1="155" x2="400" y2="185" stroke="#ef5350" stroke-width="2.5" marker-end="url(#arrowRedCG)"/>
  <text x="400" y="175" font-family="Georgia, serif" font-size="13" fill="#ef5350" text-anchor="middle" dx="80">Color Correction Pipeline</text>

  <!-- Corrected clips - all same tone -->
  <rect x="30" y="200" width="130" height="80" rx="6" fill="#c9b896" stroke="#4fc3f7" stroke-width="2"/>
  <text x="95" y="245" font-family="monospace" font-size="11" fill="#333" text-anchor="middle">Clip 1: Matched</text>

  <rect x="180" y="200" width="130" height="80" rx="6" fill="#c9b896" stroke="#4fc3f7" stroke-width="2"/>
  <text x="245" y="245" font-family="monospace" font-size="11" fill="#333" text-anchor="middle">Clip 2: Matched</text>

  <rect x="330" y="200" width="130" height="80" rx="6" fill="#c9b896" stroke="#4fc3f7" stroke-width="2"/>
  <text x="395" y="245" font-family="monospace" font-size="11" fill="#333" text-anchor="middle">Clip 3: Matched</text>

  <rect x="480" y="200" width="130" height="80" rx="6" fill="#c9b896" stroke="#4fc3f7" stroke-width="2"/>
  <text x="545" y="245" font-family="monospace" font-size="11" fill="#333" text-anchor="middle">Clip 4: Matched</text>

  <rect x="630" y="200" width="130" height="80" rx="6" fill="#c9b896" stroke="#4fc3f7" stroke-width="2"/>
  <text x="695" y="245" font-family="monospace" font-size="11" fill="#333" text-anchor="middle">Clip 5: Matched</text>

  <!-- Arrow markers -->
  <defs>
    <marker id="arrowRedCG" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="6" markerHeight="6" orient="auto">
      <path d="M 0 0 L 10 5 L 0 10 z" fill="#ef5350"/>
    </marker>
  </defs>
</svg>

The solution is a post-processing pipeline that normalizes color across all clips. This post covers every technique you need to build one.

---

## Color Space Fundamentals {#color-space-fundamentals}

Before we can match colors across clips, we need to understand color spaces. The choice of color space dramatically affects the quality of color matching algorithms. Working in the wrong space produces visible artifacts; working in the right space produces seamless results.

### RGB: The Native Space

Every digital image starts in RGB. Each pixel is a triplet $(R, G, B)$ where each channel is typically an 8-bit unsigned integer in $[0, 255]$ or a normalized float in $[0, 1]$. RGB is an additive color model --- red, green, and blue light combine to produce the full spectrum.

The problem with RGB for color matching: the channels are not perceptually independent. Changing the red channel affects perceived brightness, hue, and saturation simultaneously. A Euclidean distance in RGB space does not correspond to perceived color difference. Two colors that are far apart in RGB space might look nearly identical to humans, while two colors that are close in RGB space might look obviously different.

### HSV: Hue, Saturation, Value

HSV separates color into three intuitively meaningful components:

- **Hue** ($H$): The "color" itself --- red, orange, yellow, green, etc. Expressed as an angle in $[0, 360)$ degrees.
- **Saturation** ($S$): The "purity" of the color --- how vivid it is. In $[0, 1]$.
- **Value** ($V$): The "brightness" of the color. In $[0, 1]$.

The conversion from RGB (normalized to $[0,1]$) to HSV:

Let $C_{\max} = \max(R, G, B)$, $C_{\min} = \min(R, G, B)$, $\Delta = C_{\max} - C_{\min}$.

$$V = C_{\max}$$

$$S = \begin{cases} 0 & \text{if } C_{\max} = 0 \\ \frac{\Delta}{C_{\max}} & \text{otherwise} \end{cases}$$

$$H = \begin{cases} 0째 & \text{if } \Delta = 0 \\ 60째 \times \frac{G - B}{\Delta} \mod 6 & \text{if } C_{\max} = R \\ 60째 \times \left(\frac{B - R}{\Delta} + 2\right) & \text{if } C_{\max} = G \\ 60째 \times \left(\frac{R - G}{\Delta} + 4\right) & \text{if } C_{\max} = B \end{cases}$$

HSV is better than RGB for color grading because you can independently adjust hue, saturation, and brightness. But it still has a critical flaw: it is not perceptually uniform. Equal numerical changes in $H$, $S$, or $V$ do not correspond to equal perceived changes.

### CIELAB (LAB): The Perceptually Uniform Space

CIELAB (commonly written LAB) was designed by the International Commission on Illumination (CIE) in 1976 to be approximately perceptually uniform. A Euclidean distance of 1 in LAB space corresponds roughly to the smallest color difference a human can perceive (a "just noticeable difference" or JND).

The three channels:

- **L\***: Lightness, from 0 (black) to 100 (white)
- **a\***: Green-red axis, typically $[-128, 127]$
- **b\***: Blue-yellow axis, typically $[-128, 127]$

The conversion from RGB to LAB requires an intermediate step through CIE XYZ:

**Step 1: RGB to Linear RGB** (remove gamma correction)

For sRGB:

$$C_{\text{linear}} = \begin{cases} \frac{C_{\text{sRGB}}}{12.92} & \text{if } C_{\text{sRGB}} \leq 0.04045 \\ \left(\frac{C_{\text{sRGB}} + 0.055}{1.055}\right)^{2.4} & \text{otherwise} \end{cases}$$

where $C \in \{R, G, B\}$ and $C_{\text{sRGB}} \in [0, 1]$.

**Step 2: Linear RGB to XYZ**

Using the sRGB to XYZ matrix (D65 illuminant):

$$\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} = \begin{bmatrix} 0.4124 & 0.3576 & 0.1805 \\ 0.2126 & 0.7152 & 0.0722 \\ 0.0193 & 0.1192 & 0.9505 \end{bmatrix} \begin{bmatrix} R_{\text{linear}} \\ G_{\text{linear}} \\ B_{\text{linear}} \end{bmatrix}$$

**Step 3: XYZ to LAB**

Define the helper function:

$$f(t) = \begin{cases} t^{1/3} & \text{if } t > \delta^3 \\ \frac{t}{3\delta^2} + \frac{4}{29} & \text{otherwise} \end{cases}$$

where $\delta = \frac{6}{29}$. Then, using the D65 reference white point $(X_n, Y_n, Z_n) = (0.9505, 1.0, 1.0890)$:

$$L^* = 116 \cdot f\!\left(\frac{Y}{Y_n}\right) - 16$$

$$a^* = 500 \cdot \left[f\!\left(\frac{X}{X_n}\right) - f\!\left(\frac{Y}{Y_n}\right)\right]$$

$$b^* = 200 \cdot \left[f\!\left(\frac{Y}{Y_n}\right) - f\!\left(\frac{Z}{Z_n}\right)\right]$$

<svg viewBox="0 0 800 350" xmlns="http://www.w3.org/2000/svg" style="max-width:800px; margin: 2em auto; display: block;">
  <rect width="800" height="350" fill="white"/>
  <text x="400" y="30" font-family="Georgia, serif" font-size="16" fill="#333" text-anchor="middle" font-weight="bold">Color Space Comparison for Color Matching</text>

  <!-- RGB cube -->
  <g transform="translate(100, 70)">
    <text x="60" y="0" font-family="Georgia, serif" font-size="14" fill="#333" text-anchor="middle" font-weight="bold">RGB</text>
    <!-- Simplified cube -->
    <rect x="10" y="20" width="100" height="100" fill="none" stroke="#ef5350" stroke-width="2"/>
    <rect x="30" y="10" width="100" height="100" fill="none" stroke="#8bc34a" stroke-width="2"/>
    <line x1="10" y1="20" x2="30" y2="10" stroke="#4fc3f7" stroke-width="2"/>
    <line x1="110" y1="20" x2="130" y2="10" stroke="#4fc3f7" stroke-width="2"/>
    <line x1="10" y1="120" x2="30" y2="110" stroke="#4fc3f7" stroke-width="2"/>
    <line x1="110" y1="120" x2="130" y2="110" stroke="#4fc3f7" stroke-width="2"/>
    <text x="60" y="145" font-family="monospace" font-size="10" fill="#666" text-anchor="middle">R axis</text>
    <text x="-5" y="75" font-family="monospace" font-size="10" fill="#666" text-anchor="end">G</text>
    <text x="140" y="15" font-family="monospace" font-size="10" fill="#666">B</text>
    <text x="60" y="175" font-family="Georgia, serif" font-size="11" fill="#ef5350" text-anchor="middle">Not perceptually</text>
    <text x="60" y="190" font-family="Georgia, serif" font-size="11" fill="#ef5350" text-anchor="middle">uniform</text>
  </g>

  <!-- HSV cylinder -->
  <g transform="translate(340, 70)">
    <text x="60" y="0" font-family="Georgia, serif" font-size="14" fill="#333" text-anchor="middle" font-weight="bold">HSV</text>
    <!-- Simplified cylinder -->
    <ellipse cx="60" cy="30" rx="55" ry="18" fill="none" stroke="#ffa726" stroke-width="2"/>
    <line x1="5" y1="30" x2="5" y2="110" stroke="#ffa726" stroke-width="2"/>
    <line x1="115" y1="30" x2="115" y2="110" stroke="#ffa726" stroke-width="2"/>
    <ellipse cx="60" cy="110" rx="55" ry="18" fill="none" stroke="#ffa726" stroke-width="2"/>
    <text x="60" y="75" font-family="monospace" font-size="10" fill="#666" text-anchor="middle">H (angle)</text>
    <text x="130" y="75" font-family="monospace" font-size="10" fill="#666">S</text>
    <text x="-15" y="75" font-family="monospace" font-size="10" fill="#666">V</text>
    <text x="60" y="155" font-family="Georgia, serif" font-size="11" fill="#ffa726" text-anchor="middle">Intuitive but not</text>
    <text x="60" y="170" font-family="Georgia, serif" font-size="11" fill="#ffa726" text-anchor="middle">perceptually uniform</text>
  </g>

  <!-- LAB axes -->
  <g transform="translate(580, 70)">
    <text x="60" y="0" font-family="Georgia, serif" font-size="14" fill="#333" text-anchor="middle" font-weight="bold">CIELAB</text>
    <!-- L axis vertical -->
    <line x1="60" y1="20" x2="60" y2="120" stroke="#333" stroke-width="2" marker-end="url(#arrowBlackCG)"/>
    <text x="72" y="25" font-family="monospace" font-size="10" fill="#333">L*</text>
    <!-- a axis horizontal -->
    <line x1="60" y1="70" x2="140" y2="70" stroke="#8bc34a" stroke-width="2" marker-end="url(#arrowGreenCG)"/>
    <text x="145" y="74" font-family="monospace" font-size="10" fill="#8bc34a">a*</text>
    <!-- b axis diagonal -->
    <line x1="60" y1="70" x2="15" y2="100" stroke="#4fc3f7" stroke-width="2" marker-end="url(#arrowBlueCG)"/>
    <text x="5" y="115" font-family="monospace" font-size="10" fill="#4fc3f7">b*</text>
    <text x="60" y="155" font-family="Georgia, serif" font-size="11" fill="#8bc34a" text-anchor="middle">Perceptually uniform</text>
    <text x="60" y="170" font-family="Georgia, serif" font-size="11" fill="#8bc34a" text-anchor="middle">Best for matching</text>
  </g>

  <!-- Summary bar -->
  <rect x="40" y="260" width="720" height="60" rx="8" fill="#f5f5f5" stroke="#ddd" stroke-width="1"/>
  <text x="400" y="285" font-family="Georgia, serif" font-size="13" fill="#333" text-anchor="middle" font-weight="bold">Key insight: Always convert to LAB before color matching operations.</text>
  <text x="400" y="305" font-family="Georgia, serif" font-size="12" fill="#666" text-anchor="middle">Euclidean distance in LAB approximates perceived color difference. In RGB and HSV, it does not.</text>

  <defs>
    <marker id="arrowBlackCG" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="6" markerHeight="6" orient="auto">
      <path d="M 0 0 L 10 5 L 0 10 z" fill="#333"/>
    </marker>
    <marker id="arrowGreenCG" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="6" markerHeight="6" orient="auto">
      <path d="M 0 0 L 10 5 L 0 10 z" fill="#8bc34a"/>
    </marker>
    <marker id="arrowBlueCG" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="6" markerHeight="6" orient="auto">
      <path d="M 0 0 L 10 5 L 0 10 z" fill="#4fc3f7"/>
    </marker>
  </defs>
</svg>

### Why LAB is the Right Choice for Color Matching

When we match the color distribution of clip B to clip A, we want perceptual similarity. Two clips should "look the same" to a human viewer. LAB gives us:

1. **Perceptual uniformity**: A unit change in any direction produces a roughly equal perceived change.
2. **Separation of lightness from color**: The L\* channel is independent of the chromaticity (a\*, b\*). We can match lightness and color independently, which is often what we want.
3. **Meaningful statistics**: The mean and standard deviation of L\*, a\*, and b\* channels correspond to perceptually meaningful properties --- average brightness, average hue position, and color spread.

Here is the Python code for converting between color spaces using OpenCV:

```python
import cv2
import numpy as np

def rgb_to_lab(image_rgb: np.ndarray) -> np.ndarray:
    """Convert RGB image (uint8) to CIELAB float32."""
    # OpenCV expects BGR, not RGB
    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
    # Convert to LAB (L: 0-100, a: -128-127, b: -128-127)
    image_lab = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2LAB).astype(np.float32)
    # OpenCV LAB range for uint8: L=[0,255], a=[0,255], b=[0,255]
    # We need to rescale to true LAB range
    image_lab[:, :, 0] = image_lab[:, :, 0] * 100.0 / 255.0  # L: 0-100
    image_lab[:, :, 1] = image_lab[:, :, 1] - 128.0           # a: -128 to 127
    image_lab[:, :, 2] = image_lab[:, :, 2] - 128.0           # b: -128 to 127
    return image_lab

def lab_to_rgb(image_lab: np.ndarray) -> np.ndarray:
    """Convert CIELAB float32 to RGB uint8."""
    lab_cv = image_lab.copy()
    lab_cv[:, :, 0] = lab_cv[:, :, 0] * 255.0 / 100.0
    lab_cv[:, :, 1] = lab_cv[:, :, 1] + 128.0
    lab_cv[:, :, 2] = lab_cv[:, :, 2] + 128.0
    lab_cv = np.clip(lab_cv, 0, 255).astype(np.uint8)
    image_bgr = cv2.cvtColor(lab_cv, cv2.COLOR_LAB2BGR)
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    return image_rgb
```

---

## Color Histogram Matching {#color-histogram-matching}

Histogram matching (also called histogram specification) is the foundational algorithm for color matching. The idea: given a source image and a reference image, transform the source so that its color distribution matches the reference.

### The Mathematics of Histogram Matching

Let $p_s(v)$ be the probability density function (histogram) of pixel values in the source image for a particular channel, and $p_r(v)$ be the histogram of the reference image, where $v \in [0, V_{\max}]$.

The cumulative distribution function (CDF) for the source and reference are:

$$F_s(v) = \int_0^v p_s(u) \, du$$

$$F_r(v) = \int_0^v p_r(u) \, du$$

In discrete form (for 8-bit images with $L = 256$ levels):

$$F_s(k) = \sum_{j=0}^{k} p_s(j) = \sum_{j=0}^{k} \frac{n_s(j)}{N_s}$$

where $n_s(j)$ is the number of pixels with value $j$ in the source, and $N_s$ is the total number of pixels.

The histogram matching transformation $T$ maps each source value to a reference value:

$$T(v) = F_r^{-1}\!\left(F_s(v)\right)$$

This is the **inverse CDF mapping**. The intuition: $F_s(v)$ maps a pixel value to a cumulative probability (percentile). $F_r^{-1}$ maps that percentile back to a pixel value, but in the reference distribution. The result: a pixel at the 30th percentile in the source gets mapped to whatever value corresponds to the 30th percentile in the reference.

### Derivation

Why does this work? We want a monotonic mapping $T$ such that the transformed source has the same CDF as the reference:

$$F_r(T(v)) = F_s(v)$$

Applying $F_r^{-1}$ to both sides:

$$T(v) = F_r^{-1}(F_s(v))$$

The inverse CDF $F_r^{-1}(p)$ is defined as:

$$F_r^{-1}(p) = \inf\{v : F_r(v) \geq p\}$$

In discrete implementation, since $F_r$ is a step function, the inverse is computed by searching for the smallest value $k$ such that $F_r(k) \geq F_s(v)$.

<svg viewBox="0 0 800 450" xmlns="http://www.w3.org/2000/svg" style="max-width:800px; margin: 2em auto; display: block;">
  <rect width="800" height="450" fill="white"/>
  <text x="400" y="25" font-family="Georgia, serif" font-size="16" fill="#333" text-anchor="middle" font-weight="bold">Histogram Matching via Inverse CDF Mapping</text>

  <!-- Source histogram -->
  <g transform="translate(40, 50)">
    <text x="80" y="10" font-family="Georgia, serif" font-size="13" fill="#4fc3f7" text-anchor="middle" font-weight="bold">Source Histogram</text>
    <rect x="0" y="25" width="160" height="120" fill="none" stroke="#ddd" stroke-width="1"/>
    <!-- Bars representing a skewed-left distribution -->
    <rect x="5" y="95" width="12" height="50" fill="#4fc3f7" opacity="0.7"/>
    <rect x="20" y="65" width="12" height="80" fill="#4fc3f7" opacity="0.7"/>
    <rect x="35" y="40" width="12" height="105" fill="#4fc3f7" opacity="0.7"/>
    <rect x="50" y="50" width="12" height="95" fill="#4fc3f7" opacity="0.7"/>
    <rect x="65" y="70" width="12" height="75" fill="#4fc3f7" opacity="0.7"/>
    <rect x="80" y="90" width="12" height="55" fill="#4fc3f7" opacity="0.7"/>
    <rect x="95" y="105" width="12" height="40" fill="#4fc3f7" opacity="0.7"/>
    <rect x="110" y="115" width="12" height="30" fill="#4fc3f7" opacity="0.7"/>
    <rect x="125" y="125" width="12" height="20" fill="#4fc3f7" opacity="0.7"/>
    <rect x="140" y="130" width="12" height="15" fill="#4fc3f7" opacity="0.7"/>
    <text x="80" y="160" font-family="monospace" font-size="10" fill="#666" text-anchor="middle">pixel value</text>
  </g>

  <!-- Source CDF -->
  <g transform="translate(40, 220)">
    <text x="80" y="10" font-family="Georgia, serif" font-size="13" fill="#4fc3f7" text-anchor="middle" font-weight="bold">Source CDF F_s</text>
    <rect x="0" y="25" width="160" height="120" fill="none" stroke="#ddd" stroke-width="1"/>
    <polyline points="0,145 16,130 32,105 48,72 64,50 80,35 96,28 112,26 128,25.5 144,25.2 160,25" fill="none" stroke="#4fc3f7" stroke-width="2.5"/>
    <!-- Horizontal dashed line at a percentile -->
    <line x1="0" y1="60" x2="72" y2="60" stroke="#ef5350" stroke-width="1.5" stroke-dasharray="4,3"/>
    <line x1="72" y1="60" x2="72" y2="145" stroke="#ef5350" stroke-width="1.5" stroke-dasharray="4,3"/>
    <circle cx="72" cy="60" r="4" fill="#ef5350"/>
    <text x="-8" y="64" font-family="monospace" font-size="9" fill="#ef5350" text-anchor="end">0.6</text>
    <text x="72" y="158" font-family="monospace" font-size="9" fill="#ef5350" text-anchor="middle">v_s</text>
  </g>

  <!-- Arrow from source CDF to reference CDF -->
  <g transform="translate(200, 300)">
    <line x1="15" y1="0" x2="150" y2="0" stroke="#ef5350" stroke-width="2" marker-end="url(#arrowRedHM)"/>
    <text x="82" y="-8" font-family="Georgia, serif" font-size="11" fill="#ef5350" text-anchor="middle">Same percentile</text>
    <text x="82" y="18" font-family="Georgia, serif" font-size="11" fill="#ef5350" text-anchor="middle">F_s(v_s) = F_r(v_r)</text>
  </g>

  <!-- Reference histogram -->
  <g transform="translate(500, 50)">
    <text x="80" y="10" font-family="Georgia, serif" font-size="13" fill="#8bc34a" text-anchor="middle" font-weight="bold">Reference Histogram</text>
    <rect x="0" y="25" width="160" height="120" fill="none" stroke="#ddd" stroke-width="1"/>
    <!-- Bars representing a more centered distribution -->
    <rect x="5" y="120" width="12" height="25" fill="#8bc34a" opacity="0.7"/>
    <rect x="20" y="110" width="12" height="35" fill="#8bc34a" opacity="0.7"/>
    <rect x="35" y="90" width="12" height="55" fill="#8bc34a" opacity="0.7"/>
    <rect x="50" y="65" width="12" height="80" fill="#8bc34a" opacity="0.7"/>
    <rect x="65" y="40" width="12" height="105" fill="#8bc34a" opacity="0.7"/>
    <rect x="80" y="35" width="12" height="110" fill="#8bc34a" opacity="0.7"/>
    <rect x="95" y="55" width="12" height="90" fill="#8bc34a" opacity="0.7"/>
    <rect x="110" y="80" width="12" height="65" fill="#8bc34a" opacity="0.7"/>
    <rect x="125" y="100" width="12" height="45" fill="#8bc34a" opacity="0.7"/>
    <rect x="140" y="115" width="12" height="30" fill="#8bc34a" opacity="0.7"/>
    <text x="80" y="160" font-family="monospace" font-size="10" fill="#666" text-anchor="middle">pixel value</text>
  </g>

  <!-- Reference CDF -->
  <g transform="translate(500, 220)">
    <text x="80" y="10" font-family="Georgia, serif" font-size="13" fill="#8bc34a" text-anchor="middle" font-weight="bold">Reference CDF F_r</text>
    <rect x="0" y="25" width="160" height="120" fill="none" stroke="#ddd" stroke-width="1"/>
    <polyline points="0,145 16,138 32,125 48,105 64,78 80,52 96,35 112,29 128,26 144,25.3 160,25" fill="none" stroke="#8bc34a" stroke-width="2.5"/>
    <!-- Horizontal dashed line at same percentile -->
    <line x1="0" y1="60" x2="92" y2="60" stroke="#ef5350" stroke-width="1.5" stroke-dasharray="4,3"/>
    <line x1="92" y1="60" x2="92" y2="145" stroke="#ef5350" stroke-width="1.5" stroke-dasharray="4,3"/>
    <circle cx="92" cy="60" r="4" fill="#ef5350"/>
    <text x="-8" y="64" font-family="monospace" font-size="9" fill="#ef5350" text-anchor="end">0.6</text>
    <text x="92" y="158" font-family="monospace" font-size="9" fill="#ef5350" text-anchor="middle">v_r</text>
  </g>

  <!-- Result annotation -->
  <rect x="230" y="400" width="340" height="35" rx="6" fill="#f5f5f5" stroke="#ef5350" stroke-width="1.5"/>
  <text x="400" y="422" font-family="Georgia, serif" font-size="12" fill="#333" text-anchor="middle">T(v_s) = F_r^(-1)(F_s(v_s)) = v_r</text>

  <defs>
    <marker id="arrowRedHM" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="6" markerHeight="6" orient="auto">
      <path d="M 0 0 L 10 5 L 0 10 z" fill="#ef5350"/>
    </marker>
  </defs>
</svg>

### Per-Channel Histogram Matching in LAB

The algorithm applied per-channel in LAB space:

```python
import numpy as np
import cv2

def histogram_match_channel(source: np.ndarray, reference: np.ndarray,
                            bins: int = 256) -> np.ndarray:
    """
    Match the histogram of a single-channel source to a reference.

    Args:
        source: 1D array of source pixel values
        reference: 1D array of reference pixel values
        bins: Number of histogram bins

    Returns:
        Transformed source values
    """
    # Compute histograms
    s_values, s_bin_edges = np.histogram(source.flatten(), bins=bins, density=True)
    r_values, r_bin_edges = np.histogram(reference.flatten(), bins=bins, density=True)

    # Compute CDFs
    s_cdf = np.cumsum(s_values)
    s_cdf = s_cdf / s_cdf[-1]  # Normalize to [0, 1]

    r_cdf = np.cumsum(r_values)
    r_cdf = r_cdf / r_cdf[-1]

    # Build the mapping: for each source bin, find the reference bin
    # with the closest CDF value
    # This is the inverse CDF mapping: T(v) = F_r^{-1}(F_s(v))
    bin_centers_s = (s_bin_edges[:-1] + s_bin_edges[1:]) / 2
    bin_centers_r = (r_bin_edges[:-1] + r_bin_edges[1:]) / 2

    mapping = np.interp(s_cdf, r_cdf, bin_centers_r)

    # Apply the mapping
    source_flat = source.flatten()
    result = np.interp(source_flat, bin_centers_s, mapping)
    return result.reshape(source.shape)


def histogram_match_lab(source_rgb: np.ndarray, reference_rgb: np.ndarray) -> np.ndarray:
    """
    Match color histogram of source to reference in LAB space.

    Args:
        source_rgb: Source image (H, W, 3) uint8 RGB
        reference_rgb: Reference image (H, W, 3) uint8 RGB

    Returns:
        Color-matched source image (H, W, 3) uint8 RGB
    """
    source_lab = cv2.cvtColor(
        cv2.cvtColor(source_rgb, cv2.COLOR_RGB2BGR), cv2.COLOR_BGR2LAB
    ).astype(np.float32)
    reference_lab = cv2.cvtColor(
        cv2.cvtColor(reference_rgb, cv2.COLOR_RGB2BGR), cv2.COLOR_BGR2LAB
    ).astype(np.float32)

    matched_lab = np.zeros_like(source_lab)
    for ch in range(3):
        matched_lab[:, :, ch] = histogram_match_channel(
            source_lab[:, :, ch], reference_lab[:, :, ch]
        )

    matched_lab = np.clip(matched_lab, 0, 255).astype(np.uint8)
    matched_bgr = cv2.cvtColor(matched_lab, cv2.COLOR_LAB2BGR)
    matched_rgb = cv2.cvtColor(matched_bgr, cv2.COLOR_BGR2RGB)
    return matched_rgb
```

### Reinhard Color Transfer: A Simpler Statistical Approach

For many use cases, full histogram matching is overkill. Erik Reinhard's method (2001) is simpler and often sufficient: match the mean and standard deviation of each LAB channel.

The transformation for each channel $c \in \{L, a, b\}$:

$$\hat{x}_c = \frac{\sigma_r^c}{\sigma_s^c} \cdot (x_c - \mu_s^c) + \mu_r^c$$

where $\mu_s^c, \sigma_s^c$ are the mean and standard deviation of the source, and $\mu_r^c, \sigma_r^c$ are those of the reference.

The intuition: shift the source distribution to have the same mean as the reference, then scale it to have the same spread. This is a linear transformation that preserves the shape of the distribution while matching its first two moments.

```python
def reinhard_color_transfer(source_rgb: np.ndarray,
                            reference_rgb: np.ndarray,
                            strength: float = 1.0) -> np.ndarray:
    """
    Reinhard color transfer in LAB space.

    Args:
        source_rgb: Source image (H, W, 3) uint8 RGB
        reference_rgb: Reference image (H, W, 3) uint8 RGB
        strength: Blending strength [0, 1]. 1.0 = full transfer.

    Returns:
        Color-transferred image (H, W, 3) uint8 RGB
    """
    source_lab = cv2.cvtColor(
        cv2.cvtColor(source_rgb, cv2.COLOR_RGB2BGR), cv2.COLOR_BGR2LAB
    ).astype(np.float64)
    reference_lab = cv2.cvtColor(
        cv2.cvtColor(reference_rgb, cv2.COLOR_RGB2BGR), cv2.COLOR_BGR2LAB
    ).astype(np.float64)

    result = np.zeros_like(source_lab)

    for ch in range(3):
        s_mean = source_lab[:, :, ch].mean()
        s_std = source_lab[:, :, ch].std()
        r_mean = reference_lab[:, :, ch].mean()
        r_std = reference_lab[:, :, ch].std()

        # Avoid division by zero
        if s_std < 1e-6:
            s_std = 1e-6

        # Apply transfer: shift and scale
        transferred = (source_lab[:, :, ch] - s_mean) * (r_std / s_std) + r_mean

        # Blend with original based on strength
        result[:, :, ch] = (1 - strength) * source_lab[:, :, ch] + strength * transferred

    result = np.clip(result, 0, 255).astype(np.uint8)
    result_bgr = cv2.cvtColor(result, cv2.COLOR_LAB2BGR)
    result_rgb = cv2.cvtColor(result_bgr, cv2.COLOR_BGR2RGB)
    return result_rgb
```

### Choosing a Reference Frame

For a multi-clip project, you need a reference. Options:

| Strategy | Description | Best for |
|---|---|---|
| **First frame of first clip** | Simple and deterministic | Linear narratives |
| **Average statistics** | Compute mean/std across all clips, use as target | Balanced consistency |
| **Reference image** | External reference (color palette, mood board) | Brand-consistent content |
| **Hero shot frame** | Pick the best-looking frame across all clips | Quality-first projects |
| **Synthetic target** | Manually define target L/a/b statistics | Full creative control |

---

## Neural Style Transfer for Color Grading {#neural-style-transfer-for-color-grading}

Histogram matching handles global color statistics. But professional color grading is more nuanced --- it involves spatial relationships, tonal curves, and the way colors interact within specific luminance ranges. Neural style transfer can capture these subtleties.

### The Gram Matrix

The key mathematical object in neural style transfer is the Gram matrix. Given the feature maps of layer $l$ in a convolutional network, where $F_{ik}^l$ is the activation of filter $i$ at spatial position $k$, the Gram matrix is:

$$G_{ij}^l = \sum_k F_{ik}^l F_{jk}^l$$

The Gram matrix captures the correlations between feature channels. If features $i$ and $j$ tend to activate together, $G_{ij}^l$ is large. This encodes the "style" of an image --- the textures, patterns, and color relationships --- without encoding the spatial layout.

For color grading specifically, we care about the Gram matrices of early layers in a pretrained VGG network. Early layers capture low-level features: edges, colors, and simple textures. The Gram matrices of these layers encode color co-occurrence patterns --- which colors appear together and how they relate.

### Style Loss for Color Transfer

The style loss between a generated image $\hat{x}$ and a reference style image $x_s$ at layer $l$ is:

$$\mathcal{L}_{\text{style}}^l = \frac{1}{4 N_l^2 M_l^2} \sum_{i,j} \left(G_{ij}^l(\hat{x}) - G_{ij}^l(x_s)\right)^2$$

where $N_l$ is the number of feature maps in layer $l$ and $M_l$ is the number of spatial positions (height $\times$ width).

The total style loss across layers:

$$\mathcal{L}_{\text{style}} = \sum_l w_l \cdot \mathcal{L}_{\text{style}}^l$$

where $w_l$ are per-layer weights.

### Color-Only Style Transfer

The key insight for color grading: we want to transfer only the color/tone style, not the texture or spatial patterns. We can achieve this by:

1. **Using only early VGG layers** (conv1_1, conv1_2, conv2_1) which capture color more than texture.
2. **Adding a strong content loss** to prevent the spatial content from changing.
3. **Operating at low resolution** --- downscale to reduce texture influence, then upscale.
4. **Using luminance-only content preservation** --- allow chrominance to change freely while constraining luminance.

The combined loss:

$$\mathcal{L} = \alpha \cdot \mathcal{L}_{\text{content}} + \beta \cdot \mathcal{L}_{\text{style}} + \gamma \cdot \mathcal{L}_{\text{TV}}$$

where $\mathcal{L}_{\text{TV}}$ is total variation regularization for smoothness, and typically $\alpha \gg \beta$ to preserve content while allowing color changes.

For color-only transfer, we modify the content loss to operate only on the luminance channel:

$$\mathcal{L}_{\text{content}} = \frac{1}{2} \sum_{i,j} \left(L^*(\hat{x})_{ij} - L^*(x_c)_{ij}\right)^2$$

where $L^*$ is the lightness channel of the LAB representation and $x_c$ is the content image.

```python
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms

class ColorStyleTransfer:
    """
    Neural style transfer optimized for color grading.
    Transfers color/tone from a reference while preserving spatial content.
    """

    def __init__(self, device='cuda'):
        self.device = device
        # Load pretrained VGG19, use only early layers for color
        vgg = models.vgg19(pretrained=True).features[:12].to(device).eval()
        for param in vgg.parameters():
            param.requires_grad_(False)
        self.vgg = vgg

        # Style layers (early = color-focused)
        self.style_layers = [0, 2, 5, 7, 10]  # conv1_1 to conv2_2
        # Content layer (slightly deeper for structure)
        self.content_layer = 7  # conv2_1

        self.normalize = transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )

    def gram_matrix(self, features: torch.Tensor) -> torch.Tensor:
        """Compute Gram matrix from feature maps."""
        b, c, h, w = features.size()
        F = features.view(b, c, h * w)  # (B, C, H*W)
        G = torch.bmm(F, F.transpose(1, 2))  # (B, C, C)
        return G / (c * h * w)

    def extract_features(self, x: torch.Tensor) -> dict:
        """Extract feature maps from VGG at specified layers."""
        features = {}
        current = x
        for i, layer in enumerate(self.vgg):
            current = layer(current)
            if i in self.style_layers or i == self.content_layer:
                features[i] = current
        return features

    def transfer(self, content_img: torch.Tensor, style_img: torch.Tensor,
                 num_steps: int = 300, style_weight: float = 1e6,
                 content_weight: float = 1.0, tv_weight: float = 1e-5
                 ) -> torch.Tensor:
        """
        Transfer color style from style_img to content_img.

        Args:
            content_img: (1, 3, H, W) float tensor in [0, 1]
            style_img: (1, 3, H, W) float tensor in [0, 1]
            num_steps: Optimization steps
            style_weight: Weight for style loss (beta)
            content_weight: Weight for content loss (alpha)
            tv_weight: Weight for total variation

        Returns:
            Styled image tensor (1, 3, H, W)
        """
        content_img = content_img.to(self.device)
        style_img = style_img.to(self.device)

        # Normalize for VGG
        content_norm = self.normalize(content_img.squeeze(0)).unsqueeze(0)
        style_norm = self.normalize(style_img.squeeze(0)).unsqueeze(0)

        # Extract target features
        content_features = self.extract_features(content_norm)
        style_features = self.extract_features(style_norm)

        # Compute target Gram matrices
        style_grams = {
            layer: self.gram_matrix(style_features[layer])
            for layer in self.style_layers
        }

        # Initialize output as clone of content
        output = content_img.clone().requires_grad_(True)
        optimizer = torch.optim.Adam([output], lr=0.02)

        for step in range(num_steps):
            optimizer.zero_grad()
            output_norm = self.normalize(output.squeeze(0)).unsqueeze(0)
            output_features = self.extract_features(output_norm)

            # Content loss (preserve structure)
            content_loss = content_weight * torch.mean(
                (output_features[self.content_layer] -
                 content_features[self.content_layer]) ** 2
            )

            # Style loss (transfer color)
            style_loss = 0
            for layer in self.style_layers:
                G_output = self.gram_matrix(output_features[layer])
                G_style = style_grams[layer]
                style_loss += torch.mean((G_output - G_style) ** 2)
            style_loss *= style_weight / len(self.style_layers)

            # Total variation loss (smoothness)
            tv_loss = tv_weight * (
                torch.sum(torch.abs(output[:, :, :, :-1] - output[:, :, :, 1:])) +
                torch.sum(torch.abs(output[:, :, :-1, :] - output[:, :, 1:, :]))
            )

            total_loss = content_loss + style_loss + tv_loss
            total_loss.backward()
            optimizer.step()

            # Clamp to valid range
            with torch.no_grad():
                output.clamp_(0, 1)

            if step % 50 == 0:
                print(f"Step {step}: content={content_loss.item():.4f}, "
                      f"style={style_loss.item():.4f}, tv={tv_loss.item():.6f}")

        return output.detach()
```

### Performance Considerations

Neural style transfer for every frame of every clip is computationally expensive. Practical approaches:

1. **Transfer on keyframes only**: Apply the full neural style transfer to keyframes (e.g., every 30th frame), then interpolate the color transformation between keyframes.
2. **Compute a LUT from the transfer**: Run style transfer on a single frame, then derive a 3D LUT from the before/after pixels. Apply the LUT to all frames.
3. **Use a fast feed-forward network**: Train a small network to replicate the optimization-based transfer, then run it in a single forward pass.

Option 2 is what we will build in the next section.

---

## 3D LUT Generation {#3d-lut-generation}

A 3D Look-Up Table (LUT) is the professional standard for color grading. It is a discrete approximation of a color transformation function $f: \text{RGB} \rightarrow \text{RGB}$.

### What a 3D LUT Is Mathematically

A 3D LUT divides the RGB color cube into a regular grid of $N \times N \times N$ points. For each grid point $(R_i, G_j, B_k)$, the LUT stores the transformed color value $(R'_i, G'_j, B'_k)$.

The grid points are equally spaced. For a LUT of size $N$:

$$R_i = \frac{i}{N-1}, \quad G_j = \frac{j}{N-1}, \quad B_k = \frac{k}{N-1}$$

for $i, j, k \in \{0, 1, \ldots, N-1\}$.

Common sizes:

| LUT Size | Grid Points | Total Entries | File Size (.cube) | Precision |
|---|---|---|---|---|
| $17^3$ | 4,913 | 14,739 RGB values | ~200 KB | Adequate for most grading |
| $33^3$ | 35,937 | 107,811 RGB values | ~1.4 MB | Professional standard |
| $65^3$ | 274,625 | 823,875 RGB values | ~11 MB | Maximum precision |

### Trilinear Interpolation

For an input color $(R, G, B)$ that falls between grid points, we use trilinear interpolation. Let the input be at fractional grid coordinates:

$$r = R \cdot (N - 1), \quad g = G \cdot (N - 1), \quad b = B \cdot (N - 1)$$

The integer parts give the grid cell:

$$r_0 = \lfloor r \rfloor, \quad g_0 = \lfloor g \rfloor, \quad b_0 = \lfloor b \rfloor$$

The fractional parts give the position within the cell:

$$\delta_r = r - r_0, \quad \delta_g = g - g_0, \quad \delta_b = b - b_0$$

Trilinear interpolation combines the eight corner values of the cell:

$$f(R, G, B) = \sum_{i \in \{0,1\}} \sum_{j \in \{0,1\}} \sum_{k \in \{0,1\}} w_{ijk} \cdot \text{LUT}[r_0 + i][g_0 + j][b_0 + k]$$

where the weights are:

$$w_{ijk} = |\delta_r - i| \cdot |\delta_g - j| \cdot |\delta_b - k|$$

Wait --- that gives the complement weights. More precisely:

$$w_{ijk} = (1 - |\delta_r - i|) \cdot (1 - |\delta_g - j|) \cdot (1 - |\delta_b - k|)$$

So $w_{000} = (1 - \delta_r)(1 - \delta_g)(1 - \delta_b)$ is the weight for the lower-left-front corner, and $w_{111} = \delta_r \cdot \delta_g \cdot \delta_b$ is the weight for the upper-right-back corner.

<svg viewBox="0 0 800 400" xmlns="http://www.w3.org/2000/svg" style="max-width:800px; margin: 2em auto; display: block;">
  <rect width="800" height="400" fill="white"/>
  <text x="400" y="25" font-family="Georgia, serif" font-size="16" fill="#333" text-anchor="middle" font-weight="bold">3D LUT: Trilinear Interpolation Within a Grid Cell</text>

  <!-- 3D cube wireframe representing one cell -->
  <g transform="translate(180, 60)">
    <!-- Back face -->
    <polygon points="120,40 320,40 320,200 120,200" fill="none" stroke="#ddd" stroke-width="1"/>
    <!-- Front face -->
    <polygon points="60,80 260,80 260,240 60,240" fill="none" stroke="#333" stroke-width="1.5"/>
    <!-- Connect front to back -->
    <line x1="60" y1="80" x2="120" y2="40" stroke="#333" stroke-width="1.5"/>
    <line x1="260" y1="80" x2="320" y2="40" stroke="#333" stroke-width="1.5"/>
    <line x1="60" y1="240" x2="120" y2="200" stroke="#ddd" stroke-width="1"/>
    <line x1="260" y1="240" x2="320" y2="200" stroke="#333" stroke-width="1.5"/>

    <!-- 8 corner points with labels -->
    <circle cx="60" cy="240" r="6" fill="#4fc3f7"/>
    <text x="30" y="260" font-family="monospace" font-size="10" fill="#333">(r0,g0,b0)</text>

    <circle cx="260" cy="240" r="6" fill="#4fc3f7"/>
    <text x="262" y="260" font-family="monospace" font-size="10" fill="#333">(r0+1,g0,b0)</text>

    <circle cx="60" cy="80" r="6" fill="#4fc3f7"/>
    <text x="2" y="74" font-family="monospace" font-size="10" fill="#333">(r0,g0+1,b0)</text>

    <circle cx="260" cy="80" r="6" fill="#4fc3f7"/>

    <circle cx="120" cy="200" r="6" fill="#4fc3f7"/>
    <circle cx="320" cy="200" r="6" fill="#4fc3f7"/>
    <circle cx="120" cy="40" r="6" fill="#4fc3f7"/>
    <circle cx="320" cy="40" r="6" fill="#4fc3f7"/>
    <text x="325" y="38" font-family="monospace" font-size="10" fill="#333">(r0+1,g0+1,b0+1)</text>

    <!-- Interpolation point -->
    <circle cx="180" cy="150" r="8" fill="#ef5350"/>
    <text x="190" y="148" font-family="Georgia, serif" font-size="12" fill="#ef5350" font-weight="bold">P(r, g, b)</text>

    <!-- Dashed lines from point to edges -->
    <line x1="180" y1="150" x2="60" y2="150" stroke="#ef5350" stroke-width="1" stroke-dasharray="3,3" opacity="0.5"/>
    <line x1="180" y1="150" x2="180" y2="240" stroke="#ef5350" stroke-width="1" stroke-dasharray="3,3" opacity="0.5"/>

    <!-- Axis labels -->
    <line x1="60" y1="240" x2="260" y2="240" stroke="#ef5350" stroke-width="2" marker-end="url(#arrowRedLUT)"/>
    <text x="160" y="275" font-family="Georgia, serif" font-size="13" fill="#ef5350" text-anchor="middle">R axis</text>
    <line x1="60" y1="240" x2="60" y2="80" stroke="#8bc34a" stroke-width="2" marker-end="url(#arrowGreenLUT)"/>
    <text x="35" y="160" font-family="Georgia, serif" font-size="13" fill="#8bc34a" text-anchor="middle" transform="rotate(-90, 35, 160)">G axis</text>
    <line x1="60" y1="240" x2="120" y2="200" stroke="#4fc3f7" stroke-width="2" marker-end="url(#arrowBlueLUT)"/>
    <text x="105" y="235" font-family="Georgia, serif" font-size="13" fill="#4fc3f7">B axis</text>
  </g>

  <!-- Formula box -->
  <rect x="50" y="320" width="700" height="60" rx="8" fill="#f5f5f5" stroke="#ddd" stroke-width="1"/>
  <text x="400" y="340" font-family="monospace" font-size="11" fill="#333" text-anchor="middle">f(P) = SUM over 8 corners of w_ijk * LUT[corner]</text>
  <text x="400" y="360" font-family="monospace" font-size="11" fill="#333" text-anchor="middle">w_ijk = (1-|dr-i|)(1-|dg-j|)(1-|db-k|)   where dr,dg,db are fractional parts</text>

  <defs>
    <marker id="arrowRedLUT" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="6" markerHeight="6" orient="auto">
      <path d="M 0 0 L 10 5 L 0 10 z" fill="#ef5350"/>
    </marker>
    <marker id="arrowGreenLUT" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="6" markerHeight="6" orient="auto">
      <path d="M 0 0 L 10 5 L 0 10 z" fill="#8bc34a"/>
    </marker>
    <marker id="arrowBlueLUT" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="6" markerHeight="6" orient="auto">
      <path d="M 0 0 L 10 5 L 0 10 z" fill="#4fc3f7"/>
    </marker>
  </defs>
</svg>

### Generating a LUT from Before/After Images

Given a source frame and its color-corrected version (from histogram matching, style transfer, or manual grading), we can compute a 3D LUT that encodes the transformation. This LUT can then be applied to all other frames efficiently.

```python
import numpy as np

def generate_3d_lut(source_frame: np.ndarray, corrected_frame: np.ndarray,
                    lut_size: int = 33) -> np.ndarray:
    """
    Generate a 3D LUT from a source frame and its color-corrected version.

    The LUT maps source colors to corrected colors. For grid points that
    don't have direct pixel observations, we interpolate from nearby data.

    Args:
        source_frame: Original frame (H, W, 3) uint8 RGB
        corrected_frame: Corrected frame (H, W, 3) uint8 RGB
        lut_size: Grid size (17, 33, or 65)

    Returns:
        3D LUT as ndarray of shape (lut_size, lut_size, lut_size, 3)
    """
    # Normalize to [0, 1]
    src = source_frame.astype(np.float64) / 255.0
    cor = corrected_frame.astype(np.float64) / 255.0

    # Initialize LUT with identity mapping
    lut = np.zeros((lut_size, lut_size, lut_size, 3), dtype=np.float64)
    lut_count = np.zeros((lut_size, lut_size, lut_size), dtype=np.float64)

    for i in range(lut_size):
        for j in range(lut_size):
            for k in range(lut_size):
                lut[i, j, k] = [i / (lut_size - 1),
                                 j / (lut_size - 1),
                                 k / (lut_size - 1)]

    # For each pixel, find the nearest LUT grid point and accumulate
    # the correction
    src_flat = src.reshape(-1, 3)
    cor_flat = cor.reshape(-1, 3)

    # Map pixel values to grid indices
    grid_coords = src_flat * (lut_size - 1)
    grid_indices = np.round(grid_coords).astype(int)
    grid_indices = np.clip(grid_indices, 0, lut_size - 1)

    # Accumulate corrections at each grid point
    for idx in range(len(src_flat)):
        ri, gi, bi = grid_indices[idx]
        lut[ri, gi, bi] += cor_flat[idx]
        lut_count[ri, gi, bi] += 1

    # Average the accumulated corrections
    mask = lut_count > 0
    for c in range(3):
        lut[:, :, :, c][mask] = lut[:, :, :, c][mask] / lut_count[mask]

    # For grid points with no data, keep identity mapping
    # (already initialized)

    return lut


def apply_3d_lut(image: np.ndarray, lut: np.ndarray) -> np.ndarray:
    """
    Apply a 3D LUT to an image using trilinear interpolation.

    Args:
        image: Input image (H, W, 3) uint8 RGB
        lut: 3D LUT (N, N, N, 3) float64

    Returns:
        Color-graded image (H, W, 3) uint8 RGB
    """
    lut_size = lut.shape[0]
    img = image.astype(np.float64) / 255.0

    # Map to grid coordinates
    coords = img * (lut_size - 1)

    # Integer parts (grid cell)
    r0 = np.floor(coords[:, :, 0]).astype(int)
    g0 = np.floor(coords[:, :, 1]).astype(int)
    b0 = np.floor(coords[:, :, 2]).astype(int)

    # Clamp to valid range
    r0 = np.clip(r0, 0, lut_size - 2)
    g0 = np.clip(g0, 0, lut_size - 2)
    b0 = np.clip(b0, 0, lut_size - 2)

    # Fractional parts
    dr = coords[:, :, 0] - r0
    dg = coords[:, :, 1] - g0
    db = coords[:, :, 2] - b0

    # Trilinear interpolation
    result = np.zeros_like(img)
    for i in range(2):
        for j in range(2):
            for k in range(2):
                # Weight for this corner
                wr = (1 - dr) if i == 0 else dr
                wg = (1 - dg) if j == 0 else dg
                wb = (1 - db) if k == 0 else db
                w = wr * wg * wb

                # Look up the LUT value at this corner
                corner_val = lut[r0 + i, g0 + j, b0 + k]  # (H, W, 3)
                result += corner_val * w[:, :, np.newaxis]

    result = np.clip(result * 255, 0, 255).astype(np.uint8)
    return result


def export_cube_lut(lut: np.ndarray, filepath: str, title: str = "AI Color Grade"):
    """
    Export a 3D LUT in Adobe .cube format.

    Args:
        lut: 3D LUT (N, N, N, 3) float64 values in [0, 1]
        filepath: Output .cube file path
        title: LUT title
    """
    lut_size = lut.shape[0]

    with open(filepath, 'w') as f:
        f.write(f'TITLE "{title}"\n')
        f.write(f'LUT_3D_SIZE {lut_size}\n')
        f.write(f'DOMAIN_MIN 0.0 0.0 0.0\n')
        f.write(f'DOMAIN_MAX 1.0 1.0 1.0\n\n')

        # .cube format iterates B fastest, then G, then R
        for ri in range(lut_size):
            for gi in range(lut_size):
                for bi in range(lut_size):
                    r, g, b = lut[ri, gi, bi]
                    f.write(f'{r:.6f} {g:.6f} {b:.6f}\n')

    print(f"Exported {lut_size}^3 LUT to {filepath}")
```

---

## FFmpeg Color Grading Toolkit {#ffmpeg-color-grading-toolkit}

FFmpeg is the backbone of any video processing pipeline. It can apply LUTs, adjust curves, correct white balance, and chain multiple color operations --- all without writing a single line of Python.

### Applying a 3D LUT

```bash
# Apply a .cube LUT to a video
ffmpeg -i input.mp4 -vf "lut3d=grade.cube" -c:a copy output.mp4

# Apply with interpolation type (trilinear is default, tetrahedral is better)
ffmpeg -i input.mp4 -vf "lut3d=grade.cube:interp=tetrahedral" -c:a copy output.mp4

# Apply LUT at specific opacity (blend with original)
ffmpeg -i input.mp4 \
  -filter_complex "[0:v]split[a][b]; \
    [a]lut3d=grade.cube:interp=tetrahedral[graded]; \
    [graded][b]blend=all_mode=normal:all_opacity=0.7[out]" \
  -map "[out]" -map 0:a -c:a copy output.mp4
```

### Color Curves Adjustment

FFmpeg's `curves` filter applies tone curves to each channel:

```bash
# Boost contrast with an S-curve
ffmpeg -i input.mp4 \
  -vf "curves=preset=increase_contrast" \
  -c:a copy output.mp4

# Custom RGB curves (x/y control points, 0-1 range)
# Lift shadows, crush highlights slightly
ffmpeg -i input.mp4 \
  -vf "curves=r='0/0.05 0.25/0.3 0.5/0.5 0.75/0.72 1/0.95':\
              g='0/0.05 0.25/0.3 0.5/0.5 0.75/0.72 1/0.95':\
              b='0/0.05 0.25/0.3 0.5/0.5 0.75/0.72 1/0.95'" \
  -c:a copy output.mp4

# Warm the image (boost red shadows, reduce blue shadows)
ffmpeg -i input.mp4 \
  -vf "curves=r='0/0.05 0.5/0.52 1/1':\
              b='0/0 0.5/0.48 1/0.95'" \
  -c:a copy output.mp4
```

### Levels Adjustment

The `levels` filter (implemented via `colorlevels`) adjusts black point, white point, and gamma:

```bash
# Set input black/white points (clip shadows and highlights)
ffmpeg -i input.mp4 \
  -vf "colorlevels=rimin=0.05:gimin=0.05:bimin=0.05:\
                    rimax=0.95:gimax=0.95:bimax=0.95" \
  -c:a copy output.mp4

# Adjust output levels (lift blacks, reduce peak white)
ffmpeg -i input.mp4 \
  -vf "colorlevels=romin=0.05:gomin=0.05:bomin=0.05:\
                    romax=0.92:gomax=0.92:bomax=0.92" \
  -c:a copy output.mp4
```

### White Balance Correction

White balance correction shifts colors so that neutral colors (grays) appear truly neutral. In FFmpeg, the `colortemperature` filter is the most direct approach:

```bash
# Warm up the image (increase color temperature)
ffmpeg -i input.mp4 -vf "colortemperature=temperature=6500" -c:a copy output.mp4

# Cool down the image
ffmpeg -i input.mp4 -vf "colortemperature=temperature=3200" -c:a copy output.mp4

# Manual white balance via colorbalance
ffmpeg -i input.mp4 \
  -vf "colorbalance=rs=0.1:gs=0:bs=-0.1:\
                    rm=0.05:gm=0:bm=-0.05:\
                    rh=0:gh=0:bh=0" \
  -c:a copy output.mp4
```

### Complete Filter Chain for Color Consistency

Here is a comprehensive FFmpeg filter chain that corrects white balance, adjusts levels, applies a LUT, and adds a subtle vignette --- all in one pass:

```bash
ffmpeg -i input.mp4 \
  -vf "\
    colorbalance=rs=0.05:bs=-0.05,\
    colorlevels=rimin=0.02:gimin=0.02:bimin=0.02:\
                rimax=0.98:gimax=0.98:bimax=0.98,\
    eq=brightness=0.02:contrast=1.05:saturation=1.1,\
    lut3d=grade.cube:interp=tetrahedral,\
    vignette=PI/4\
  " \
  -c:v libx264 -crf 18 -preset medium \
  -c:a copy \
  output.mp4
```

### Batch Processing Multiple Clips

```bash
#!/bin/bash
# Apply the same color grade to all clips in a directory

LUT_FILE="grade.cube"
INPUT_DIR="./clips"
OUTPUT_DIR="./graded"
mkdir -p "$OUTPUT_DIR"

for clip in "$INPUT_DIR"/*.mp4; do
    filename=$(basename "$clip")
    echo "Grading: $filename"
    ffmpeg -i "$clip" \
      -vf "lut3d=$LUT_FILE:interp=tetrahedral" \
      -c:v libx264 -crf 18 -preset medium \
      -c:a copy \
      "$OUTPUT_DIR/$filename" \
      -y -loglevel warning
done

echo "All clips graded."
```

---

## Automated Color Consistency Pipeline {#automated-color-consistency-pipeline}

Now we assemble everything into a complete automated pipeline. Given N clips from AI generation, the pipeline produces N color-consistent clips.

<svg viewBox="0 0 800 520" xmlns="http://www.w3.org/2000/svg" style="max-width:800px; margin: 2em auto; display: block;">
  <rect width="800" height="520" fill="white"/>
  <text x="400" y="25" font-family="Georgia, serif" font-size="16" fill="#333" text-anchor="middle" font-weight="bold">Automated Color Consistency Pipeline</text>

  <!-- Step 1 -->
  <rect x="20" y="50" width="180" height="50" rx="8" fill="#4fc3f7" opacity="0.15" stroke="#4fc3f7" stroke-width="2"/>
  <text x="110" y="72" font-family="Georgia, serif" font-size="12" fill="#333" text-anchor="middle" font-weight="bold">1. Extract Frames</text>
  <text x="110" y="88" font-family="monospace" font-size="10" fill="#666" text-anchor="middle">1 frame/sec per clip</text>

  <!-- Arrow -->
  <line x1="200" y1="75" x2="230" y2="75" stroke="#333" stroke-width="1.5" marker-end="url(#arrowPipe)"/>

  <!-- Step 2 -->
  <rect x="230" y="50" width="180" height="50" rx="8" fill="#8bc34a" opacity="0.15" stroke="#8bc34a" stroke-width="2"/>
  <text x="320" y="72" font-family="Georgia, serif" font-size="12" fill="#333" text-anchor="middle" font-weight="bold">2. Color Statistics</text>
  <text x="320" y="88" font-family="monospace" font-size="10" fill="#666" text-anchor="middle">LAB mean, std, hist</text>

  <!-- Arrow -->
  <line x1="410" y1="75" x2="440" y2="75" stroke="#333" stroke-width="1.5" marker-end="url(#arrowPipe)"/>

  <!-- Step 3 -->
  <rect x="440" y="50" width="180" height="50" rx="8" fill="#ffa726" opacity="0.15" stroke="#ffa726" stroke-width="2"/>
  <text x="530" y="72" font-family="Georgia, serif" font-size="12" fill="#333" text-anchor="middle" font-weight="bold">3. Compute Target</text>
  <text x="530" y="88" font-family="monospace" font-size="10" fill="#666" text-anchor="middle">Average or reference</text>

  <!-- Arrow -->
  <line x1="620" y1="75" x2="650" y2="75" stroke="#333" stroke-width="1.5" marker-end="url(#arrowPipe)"/>

  <!-- Step 4 -->
  <rect x="650" y="50" width="130" height="50" rx="8" fill="#ef5350" opacity="0.15" stroke="#ef5350" stroke-width="2"/>
  <text x="715" y="72" font-family="Georgia, serif" font-size="12" fill="#333" text-anchor="middle" font-weight="bold">4. Generate LUT</text>
  <text x="715" y="88" font-family="monospace" font-size="10" fill="#666" text-anchor="middle">Per clip</text>

  <!-- Arrow down from step 4 -->
  <line x1="715" y1="100" x2="715" y2="130" stroke="#333" stroke-width="1.5" marker-end="url(#arrowPipe)"/>

  <!-- Step 5 -->
  <rect x="600" y="130" width="230" height="50" rx="8" fill="#4fc3f7" opacity="0.15" stroke="#4fc3f7" stroke-width="2"/>
  <text x="715" y="152" font-family="Georgia, serif" font-size="12" fill="#333" text-anchor="middle" font-weight="bold">5. Apply LUT (FFmpeg)</text>
  <text x="715" y="168" font-family="monospace" font-size="10" fill="#666" text-anchor="middle">Batch process all clips</text>

  <!-- Arrow down from step 5 -->
  <line x1="715" y1="180" x2="715" y2="210" stroke="#333" stroke-width="1.5" marker-end="url(#arrowPipe)"/>

  <!-- Step 6 -->
  <rect x="600" y="210" width="230" height="50" rx="8" fill="#8bc34a" opacity="0.15" stroke="#8bc34a" stroke-width="2"/>
  <text x="715" y="232" font-family="Georgia, serif" font-size="12" fill="#333" text-anchor="middle" font-weight="bold">6. Verify Consistency</text>
  <text x="715" y="248" font-family="monospace" font-size="10" fill="#666" text-anchor="middle">CIEDE2000 delta check</text>

  <!-- Feedback loop if verification fails -->
  <line x1="600" y1="235" x2="530" y2="235" stroke="#ef5350" stroke-width="1.5" stroke-dasharray="5,3"/>
  <line x1="530" y1="235" x2="530" y2="100" stroke="#ef5350" stroke-width="1.5" stroke-dasharray="5,3" marker-end="url(#arrowRedPipe)"/>
  <text x="500" y="170" font-family="Georgia, serif" font-size="10" fill="#ef5350" text-anchor="middle" transform="rotate(-90, 500, 170)">Retry if delta > threshold</text>

  <!-- Detailed breakdown below -->
  <line x1="0" y1="290" x2="800" y2="290" stroke="#eee" stroke-width="1"/>
  <text x="400" y="315" font-family="Georgia, serif" font-size="14" fill="#333" text-anchor="middle" font-weight="bold">Per-Clip Processing Detail</text>

  <!-- Per clip detail boxes -->
  <rect x="30" y="335" width="140" height="70" rx="6" fill="#f9f9f9" stroke="#ddd" stroke-width="1"/>
  <text x="100" y="355" font-family="monospace" font-size="10" fill="#333" text-anchor="middle">clip_i.mp4</text>
  <text x="100" y="375" font-family="monospace" font-size="9" fill="#666" text-anchor="middle">L*: 52.3 +/- 18.7</text>
  <text x="100" y="390" font-family="monospace" font-size="9" fill="#666" text-anchor="middle">a*: 3.1  b*: 12.4</text>

  <line x1="170" y1="370" x2="210" y2="370" stroke="#333" stroke-width="1.5" marker-end="url(#arrowPipe)"/>

  <rect x="210" y="335" width="140" height="70" rx="6" fill="#f9f9f9" stroke="#ffa726" stroke-width="1.5"/>
  <text x="280" y="355" font-family="monospace" font-size="10" fill="#333" text-anchor="middle">Reinhard Transfer</text>
  <text x="280" y="375" font-family="monospace" font-size="9" fill="#666" text-anchor="middle">Target L*: 55.0</text>
  <text x="280" y="390" font-family="monospace" font-size="9" fill="#666" text-anchor="middle">Target a*: 2.0 b*: 8.0</text>

  <line x1="350" y1="370" x2="390" y2="370" stroke="#333" stroke-width="1.5" marker-end="url(#arrowPipe)"/>

  <rect x="390" y="335" width="140" height="70" rx="6" fill="#f9f9f9" stroke="#ef5350" stroke-width="1.5"/>
  <text x="460" y="355" font-family="monospace" font-size="10" fill="#333" text-anchor="middle">Generate LUT</text>
  <text x="460" y="375" font-family="monospace" font-size="9" fill="#666" text-anchor="middle">33^3 grid</text>
  <text x="460" y="390" font-family="monospace" font-size="9" fill="#666" text-anchor="middle">clip_i.cube</text>

  <line x1="530" y1="370" x2="570" y2="370" stroke="#333" stroke-width="1.5" marker-end="url(#arrowPipe)"/>

  <rect x="570" y="335" width="140" height="70" rx="6" fill="#f9f9f9" stroke="#4fc3f7" stroke-width="1.5"/>
  <text x="640" y="355" font-family="monospace" font-size="10" fill="#333" text-anchor="middle">Apply via FFmpeg</text>
  <text x="640" y="375" font-family="monospace" font-size="9" fill="#666" text-anchor="middle">lut3d=clip_i.cube</text>
  <text x="640" y="390" font-family="monospace" font-size="9" fill="#666" text-anchor="middle">tetrahedral interp</text>

  <!-- Output -->
  <line x1="710" y1="370" x2="740" y2="370" stroke="#333" stroke-width="1.5" marker-end="url(#arrowPipe)"/>
  <rect x="740" y="348" width="50" height="44" rx="6" fill="#8bc34a" opacity="0.2" stroke="#8bc34a" stroke-width="2"/>
  <text x="765" y="375" font-family="monospace" font-size="10" fill="#333" text-anchor="middle">OK</text>

  <defs>
    <marker id="arrowPipe" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="6" markerHeight="6" orient="auto">
      <path d="M 0 0 L 10 5 L 0 10 z" fill="#333"/>
    </marker>
    <marker id="arrowRedPipe" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="6" markerHeight="6" orient="auto">
      <path d="M 0 0 L 10 5 L 0 10 z" fill="#ef5350"/>
    </marker>
  </defs>
</svg>

### Full Pipeline Code

```python
import os
import subprocess
import json
import numpy as np
import cv2
from pathlib import Path
from dataclasses import dataclass
from typing import Optional


@dataclass
class ColorProfile:
    """Color statistics for a clip, computed in LAB space."""
    l_mean: float
    l_std: float
    a_mean: float
    a_std: float
    b_mean: float
    b_std: float
    l_histogram: np.ndarray  # 256 bins
    a_histogram: np.ndarray
    b_histogram: np.ndarray


def extract_frames(video_path: str, output_dir: str,
                   fps: float = 1.0) -> list[str]:
    """Extract frames from a video at the given FPS."""
    os.makedirs(output_dir, exist_ok=True)
    pattern = os.path.join(output_dir, "frame_%04d.png")

    cmd = [
        "ffmpeg", "-i", video_path,
        "-vf", f"fps={fps}",
        "-q:v", "2",
        pattern,
        "-y", "-loglevel", "warning"
    ]
    subprocess.run(cmd, check=True)

    frames = sorted([
        os.path.join(output_dir, f) for f in os.listdir(output_dir)
        if f.endswith('.png')
    ])
    return frames


def compute_color_profile(frames: list[str]) -> ColorProfile:
    """Compute LAB color statistics from a list of frame images."""
    all_l, all_a, all_b = [], [], []

    for frame_path in frames:
        img = cv2.imread(frame_path)
        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB).astype(np.float32)
        all_l.append(lab[:, :, 0].flatten())
        all_a.append(lab[:, :, 1].flatten())
        all_b.append(lab[:, :, 2].flatten())

    l_all = np.concatenate(all_l)
    a_all = np.concatenate(all_a)
    b_all = np.concatenate(all_b)

    return ColorProfile(
        l_mean=float(np.mean(l_all)),
        l_std=float(np.std(l_all)),
        a_mean=float(np.mean(a_all)),
        a_std=float(np.std(a_all)),
        b_mean=float(np.mean(b_all)),
        b_std=float(np.std(b_all)),
        l_histogram=np.histogram(l_all, bins=256, range=(0, 255))[0],
        a_histogram=np.histogram(a_all, bins=256, range=(0, 255))[0],
        b_histogram=np.histogram(b_all, bins=256, range=(0, 255))[0],
    )


def compute_target_profile(profiles: list[ColorProfile],
                           method: str = "average") -> ColorProfile:
    """
    Compute the target color profile from multiple clip profiles.

    Methods:
        'average': Average statistics across all clips
        'first': Use the first clip as reference
        'brightest': Use the clip with highest mean L*
    """
    if method == "first":
        return profiles[0]
    elif method == "brightest":
        idx = np.argmax([p.l_mean for p in profiles])
        return profiles[idx]
    elif method == "average":
        return ColorProfile(
            l_mean=np.mean([p.l_mean for p in profiles]),
            l_std=np.mean([p.l_std for p in profiles]),
            a_mean=np.mean([p.a_mean for p in profiles]),
            a_std=np.mean([p.a_std for p in profiles]),
            b_mean=np.mean([p.b_mean for p in profiles]),
            b_std=np.mean([p.b_std for p in profiles]),
            l_histogram=np.mean([p.l_histogram for p in profiles], axis=0),
            a_histogram=np.mean([p.a_histogram for p in profiles], axis=0),
            b_histogram=np.mean([p.b_histogram for p in profiles], axis=0),
        )
    else:
        raise ValueError(f"Unknown method: {method}")


def color_correct_frame(frame: np.ndarray, source_profile: ColorProfile,
                        target_profile: ColorProfile) -> np.ndarray:
    """
    Apply Reinhard color transfer to match target profile.
    Frame is BGR uint8, returns BGR uint8.
    """
    lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB).astype(np.float64)

    # L channel
    if source_profile.l_std > 1e-6:
        lab[:, :, 0] = ((lab[:, :, 0] - source_profile.l_mean)
                        * (target_profile.l_std / source_profile.l_std)
                        + target_profile.l_mean)

    # a channel
    if source_profile.a_std > 1e-6:
        lab[:, :, 1] = ((lab[:, :, 1] - source_profile.a_mean)
                        * (target_profile.a_std / source_profile.a_std)
                        + target_profile.a_mean)

    # b channel
    if source_profile.b_std > 1e-6:
        lab[:, :, 2] = ((lab[:, :, 2] - source_profile.b_mean)
                        * (target_profile.b_std / source_profile.b_std)
                        + target_profile.b_mean)

    lab = np.clip(lab, 0, 255).astype(np.uint8)
    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)


def generate_lut_for_clip(source_frame_path: str,
                          source_profile: ColorProfile,
                          target_profile: ColorProfile,
                          lut_size: int = 33) -> np.ndarray:
    """
    Generate a 3D LUT by applying Reinhard transfer to a representative frame
    and deriving the LUT from the before/after pixel mapping.
    """
    source = cv2.imread(source_frame_path)
    corrected = color_correct_frame(source, source_profile, target_profile)

    # Generate LUT from the transformation
    lut = generate_3d_lut(
        cv2.cvtColor(source, cv2.COLOR_BGR2RGB),
        cv2.cvtColor(corrected, cv2.COLOR_BGR2RGB),
        lut_size=lut_size
    )
    return lut


def run_pipeline(clip_paths: list[str], output_dir: str,
                 target_method: str = "average",
                 lut_size: int = 33) -> list[str]:
    """
    Complete color consistency pipeline.

    Args:
        clip_paths: List of input video file paths
        output_dir: Directory for output files
        target_method: 'average', 'first', or 'brightest'
        lut_size: 3D LUT grid size

    Returns:
        List of color-corrected video file paths
    """
    os.makedirs(output_dir, exist_ok=True)
    temp_dir = os.path.join(output_dir, "_temp")
    os.makedirs(temp_dir, exist_ok=True)

    # Step 1: Extract frames from each clip
    print("Step 1: Extracting frames...")
    all_frames = {}
    for i, clip_path in enumerate(clip_paths):
        frame_dir = os.path.join(temp_dir, f"clip_{i}_frames")
        frames = extract_frames(clip_path, frame_dir, fps=1.0)
        all_frames[clip_path] = frames
        print(f"  Clip {i}: extracted {len(frames)} frames")

    # Step 2: Compute color profiles
    print("Step 2: Computing color profiles...")
    profiles = {}
    for clip_path, frames in all_frames.items():
        profile = compute_color_profile(frames)
        profiles[clip_path] = profile
        print(f"  {os.path.basename(clip_path)}: "
              f"L*={profile.l_mean:.1f}+/-{profile.l_std:.1f} "
              f"a*={profile.a_mean:.1f} b*={profile.b_mean:.1f}")

    # Step 3: Compute target profile
    print(f"Step 3: Computing target profile (method={target_method})...")
    target = compute_target_profile(list(profiles.values()), method=target_method)
    print(f"  Target: L*={target.l_mean:.1f}+/-{target.l_std:.1f} "
          f"a*={target.a_mean:.1f} b*={target.b_mean:.1f}")

    # Step 4: Generate LUTs and apply
    print("Step 4: Generating LUTs and applying...")
    output_paths = []
    for i, clip_path in enumerate(clip_paths):
        # Generate LUT
        representative_frame = all_frames[clip_path][0]
        lut = generate_lut_for_clip(
            representative_frame, profiles[clip_path], target, lut_size
        )

        # Export LUT
        lut_path = os.path.join(temp_dir, f"clip_{i}.cube")
        export_cube_lut(lut, lut_path, title=f"Clip {i} Color Match")

        # Apply via FFmpeg
        output_path = os.path.join(
            output_dir, f"graded_{os.path.basename(clip_path)}"
        )
        cmd = [
            "ffmpeg", "-i", clip_path,
            "-vf", f"lut3d={lut_path}:interp=tetrahedral",
            "-c:v", "libx264", "-crf", "18", "-preset", "medium",
            "-c:a", "copy",
            output_path,
            "-y", "-loglevel", "warning"
        ]
        subprocess.run(cmd, check=True)
        output_paths.append(output_path)
        print(f"  Clip {i}: graded -> {output_path}")

    # Step 5: Verify consistency
    print("Step 5: Verifying consistency...")
    verify_results = verify_consistency(output_paths, temp_dir)
    for clip_name, delta_e in verify_results.items():
        status = "PASS" if delta_e < 5.0 else "FAIL"
        print(f"  {clip_name}: mean deltaE = {delta_e:.2f} [{status}]")

    return output_paths


def verify_consistency(clip_paths: list[str], temp_dir: str) -> dict:
    """Check color consistency by comparing profiles of graded clips."""
    profiles = []
    for i, clip_path in enumerate(clip_paths):
        frame_dir = os.path.join(temp_dir, f"verify_{i}")
        frames = extract_frames(clip_path, frame_dir, fps=1.0)
        profile = compute_color_profile(frames)
        profiles.append((os.path.basename(clip_path), profile))

    # Compute pairwise delta E between clip mean colors
    results = {}
    for i, (name_i, prof_i) in enumerate(profiles):
        deltas = []
        for j, (name_j, prof_j) in enumerate(profiles):
            if i == j:
                continue
            # Simplified delta E using LAB Euclidean distance
            delta = np.sqrt(
                (prof_i.l_mean - prof_j.l_mean) ** 2 +
                (prof_i.a_mean - prof_j.a_mean) ** 2 +
                (prof_i.b_mean - prof_j.b_mean) ** 2
            )
            deltas.append(delta)
        results[name_i] = np.mean(deltas)

    return results
```

---

## Perceptual Color Difference: CIEDE2000 {#perceptual-color-difference-ciede2000}

The simplified Euclidean distance in LAB space is a reasonable first approximation, but for serious quality control, we need the CIEDE2000 formula. It is the most accurate perceptual color difference metric, incorporating corrections for lightness, chroma, and hue that LAB's Euclidean distance misses.

### The CIEDE2000 Formula

Given two colors in LAB space: $(L_1^*, a_1^*, b_1^*)$ and $(L_2^*, a_2^*, b_2^*)$.

**Step 1: Compute chroma and hue in CIELAB.**

$$C_i^* = \sqrt{a_i^{*2} + b_i^{*2}}, \quad i \in \{1, 2\}$$

$$\bar{C}^* = \frac{C_1^* + C_2^*}{2}$$

**Step 2: Compute the a' adjustment.**

$$G = 0.5 \left(1 - \sqrt{\frac{\bar{C}^{*7}}{\bar{C}^{*7} + 25^7}}\right)$$

$$a_i' = a_i^* (1 + G)$$

**Step 3: Recompute chroma and hue with adjusted a'.**

$$C_i' = \sqrt{a_i'^2 + b_i^{*2}}$$

$$h_i' = \text{atan2}(b_i^*, a_i') \mod 360째$$

**Step 4: Compute differences.**

$$\Delta L' = L_2^* - L_1^*$$

$$\Delta C' = C_2' - C_1'$$

$$\Delta h' = \begin{cases}
h_2' - h_1' & \text{if } |h_2' - h_1'| \leq 180째 \\
h_2' - h_1' - 360째 & \text{if } h_2' - h_1' > 180째 \\
h_2' - h_1' + 360째 & \text{if } h_2' - h_1' < -180째
\end{cases}$$

$$\Delta H' = 2\sqrt{C_1' C_2'} \sin\!\left(\frac{\Delta h'}{2}\right)$$

**Step 5: Compute weighting functions.**

$$\bar{L}' = \frac{L_1^* + L_2^*}{2}$$

$$\bar{C}' = \frac{C_1' + C_2'}{2}$$

$$\bar{h}' = \begin{cases}
\frac{h_1' + h_2'}{2} & \text{if } |h_1' - h_2'| \leq 180째 \\
\frac{h_1' + h_2' + 360째}{2} & \text{if } |h_1' - h_2'| > 180째 \text{ and } h_1' + h_2' < 360째 \\
\frac{h_1' + h_2' - 360째}{2} & \text{otherwise}
\end{cases}$$

$$T = 1 - 0.17\cos(\bar{h}' - 30째) + 0.24\cos(2\bar{h}') + 0.32\cos(3\bar{h}' + 6째) - 0.20\cos(4\bar{h}' - 63째)$$

$$S_L = 1 + \frac{0.015(\bar{L}' - 50)^2}{\sqrt{20 + (\bar{L}' - 50)^2}}$$

$$S_C = 1 + 0.045\bar{C}'$$

$$S_H = 1 + 0.015\bar{C}' T$$

$$R_T = -\sin(2\Delta\theta) \cdot R_C$$

where:

$$\Delta\theta = 30째 \exp\!\left(-\left(\frac{\bar{h}' - 275째}{25째}\right)^2\right)$$

$$R_C = 2\sqrt{\frac{\bar{C}'^7}{\bar{C}'^7 + 25^7}}$$

**Step 6: The final CIEDE2000 distance.**

$$\Delta E_{00} = \sqrt{\left(\frac{\Delta L'}{k_L S_L}\right)^2 + \left(\frac{\Delta C'}{k_C S_C}\right)^2 + \left(\frac{\Delta H'}{k_H S_H}\right)^2 + R_T \frac{\Delta C'}{k_C S_C} \frac{\Delta H'}{k_H S_H}}$$

The parametric factors $k_L, k_C, k_H$ are typically all set to 1 for standard conditions.

### Interpreting CIEDE2000 Values

| $\Delta E_{00}$ | Perceptual Meaning |
|---|---|
| 0 -- 1 | Not perceptible to the human eye |
| 1 -- 2 | Perceptible through close observation |
| 2 -- 3.5 | Perceptible at a glance |
| 3.5 -- 5 | Clear difference, still acceptable for some use cases |
| > 5 | Obvious difference, unacceptable for matched content |

For AI video consistency, we target $\Delta E_{00} < 3$ between the mean colors of consecutive clips.

### Implementation

```python
import numpy as np

def ciede2000(lab1: np.ndarray, lab2: np.ndarray,
              kL: float = 1.0, kC: float = 1.0, kH: float = 1.0) -> float:
    """
    Compute the CIEDE2000 color difference between two LAB colors.

    Args:
        lab1: (L*, a*, b*) of color 1
        lab2: (L*, a*, b*) of color 2
        kL, kC, kH: Parametric weighting factors (default 1.0)

    Returns:
        Delta E_00 value
    """
    L1, a1, b1 = lab1
    L2, a2, b2 = lab2

    # Step 1: CIELAB chroma
    C1_star = np.sqrt(a1**2 + b1**2)
    C2_star = np.sqrt(a2**2 + b2**2)
    C_bar_star = (C1_star + C2_star) / 2.0

    # Step 2: a' adjustment
    C_bar_star_7 = C_bar_star**7
    G = 0.5 * (1 - np.sqrt(C_bar_star_7 / (C_bar_star_7 + 25**7)))
    a1_prime = a1 * (1 + G)
    a2_prime = a2 * (1 + G)

    # Step 3: Recompute chroma and hue
    C1_prime = np.sqrt(a1_prime**2 + b1**2)
    C2_prime = np.sqrt(a2_prime**2 + b2**2)

    h1_prime = np.degrees(np.arctan2(b1, a1_prime)) % 360
    h2_prime = np.degrees(np.arctan2(b2, a2_prime)) % 360

    # Step 4: Differences
    dL_prime = L2 - L1
    dC_prime = C2_prime - C1_prime

    dh_prime = h2_prime - h1_prime
    if abs(dh_prime) > 180:
        if dh_prime > 180:
            dh_prime -= 360
        else:
            dh_prime += 360

    dH_prime = 2 * np.sqrt(C1_prime * C2_prime) * np.sin(np.radians(dh_prime / 2))

    # Step 5: Weighting functions
    L_bar_prime = (L1 + L2) / 2.0
    C_bar_prime = (C1_prime + C2_prime) / 2.0

    h_diff = abs(h1_prime - h2_prime)
    if h_diff <= 180:
        h_bar_prime = (h1_prime + h2_prime) / 2.0
    else:
        if h1_prime + h2_prime < 360:
            h_bar_prime = (h1_prime + h2_prime + 360) / 2.0
        else:
            h_bar_prime = (h1_prime + h2_prime - 360) / 2.0

    T = (1
         - 0.17 * np.cos(np.radians(h_bar_prime - 30))
         + 0.24 * np.cos(np.radians(2 * h_bar_prime))
         + 0.32 * np.cos(np.radians(3 * h_bar_prime + 6))
         - 0.20 * np.cos(np.radians(4 * h_bar_prime - 63)))

    SL = 1 + 0.015 * (L_bar_prime - 50)**2 / np.sqrt(20 + (L_bar_prime - 50)**2)
    SC = 1 + 0.045 * C_bar_prime
    SH = 1 + 0.015 * C_bar_prime * T

    d_theta = 30 * np.exp(-((h_bar_prime - 275) / 25)**2)
    C_bar_prime_7 = C_bar_prime**7
    RC = 2 * np.sqrt(C_bar_prime_7 / (C_bar_prime_7 + 25**7))
    RT = -np.sin(np.radians(2 * d_theta)) * RC

    # Step 6: Final distance
    term_L = dL_prime / (kL * SL)
    term_C = dC_prime / (kC * SC)
    term_H = dH_prime / (kH * SH)

    dE00 = np.sqrt(term_L**2 + term_C**2 + term_H**2 + RT * term_C * term_H)
    return dE00


def measure_clip_consistency(clip_frames_a: list[np.ndarray],
                             clip_frames_b: list[np.ndarray]) -> dict:
    """
    Measure color consistency between two clips.

    Returns mean and max CIEDE2000 delta E between
    corresponding frame mean colors.
    """
    def mean_lab(frames):
        labs = []
        for f in frames:
            lab = cv2.cvtColor(f, cv2.COLOR_BGR2LAB).astype(np.float64)
            # OpenCV LAB: L=[0,255], a=[0,255], b=[0,255]
            # Convert to standard: L=[0,100], a=[-128,127], b=[-128,127]
            lab[:, :, 0] *= 100.0 / 255.0
            lab[:, :, 1] -= 128.0
            lab[:, :, 2] -= 128.0
            labs.append(lab.mean(axis=(0, 1)))
        return np.array(labs)

    labs_a = mean_lab(clip_frames_a)
    labs_b = mean_lab(clip_frames_b)

    # Compare overall mean colors
    mean_a = labs_a.mean(axis=0)
    mean_b = labs_b.mean(axis=0)
    overall_de = ciede2000(mean_a, mean_b)

    # Compare frame-by-frame (up to min length)
    n = min(len(labs_a), len(labs_b))
    frame_deltas = [ciede2000(labs_a[i], labs_b[i]) for i in range(n)]

    return {
        "overall_delta_e": overall_de,
        "mean_frame_delta_e": np.mean(frame_deltas),
        "max_frame_delta_e": np.max(frame_deltas),
        "frame_deltas": frame_deltas,
    }
```

---

## Putting It All Together {#putting-it-all-together}

Here is the complete workflow for achieving color consistency in an AI video pipeline:

### Decision Matrix: Which Technique to Use

| Scenario | Technique | When to Use |
|---|---|---|
| Quick consistency fix | Reinhard Transfer | Same scene, similar lighting |
| Professional-grade match | Histogram Matching in LAB | Different scenes, need precise match |
| Artistic color grading | Neural Style Transfer | Match a reference film's look |
| Production pipeline | 3D LUT via FFmpeg | High volume, need speed |
| Quality validation | CIEDE2000 | Always, as final check |

### End-to-End Example

```python
# Complete example: color-grade 5 AI-generated clips for consistency

clip_paths = [
    "clips/shot_01_establishing.mp4",
    "clips/shot_02_medium.mp4",
    "clips/shot_03_closeup.mp4",
    "clips/shot_04_reaction.mp4",
    "clips/shot_05_wide.mp4",
]

# Run the pipeline
graded_paths = run_pipeline(
    clip_paths=clip_paths,
    output_dir="output/graded",
    target_method="average",  # Use average color profile as target
    lut_size=33,              # Professional-grade LUT resolution
)

# Concatenate graded clips with crossfade transitions
concat_cmd = [
    "ffmpeg",
    "-i", graded_paths[0],
    "-i", graded_paths[1],
    "-i", graded_paths[2],
    "-i", graded_paths[3],
    "-i", graded_paths[4],
    "-filter_complex",
    "[0:v][1:v]xfade=transition=fade:duration=0.5:offset=5.5[v01];"
    "[v01][2:v]xfade=transition=fade:duration=0.5:offset=11[v012];"
    "[v012][3:v]xfade=transition=fade:duration=0.5:offset=16.5[v0123];"
    "[v0123][4:v]xfade=transition=fade:duration=0.5:offset=22[vfinal]",
    "-map", "[vfinal]",
    "-c:v", "libx264", "-crf", "18",
    "output/final_sequence.mp4",
    "-y"
]
subprocess.run(concat_cmd, check=True)
print("Final color-consistent sequence: output/final_sequence.mp4")
```

### Performance Benchmarks

For a typical 5-clip project (each clip 6 seconds at 24fps, 1080p):

| Step | Time | Notes |
|---|---|---|
| Frame extraction | ~2s per clip | 6 frames at 1fps |
| Color profiling | ~1s per clip | LAB statistics |
| LUT generation | ~3s per clip | 33x33x33 grid |
| FFmpeg grading | ~5s per clip | Hardware-accelerated |
| Verification | ~3s total | Spot-check frames |
| **Total** | **~60s for 5 clips** | Single-threaded |

With parallelization (clips are independent), wall-clock time drops to under 15 seconds.

### Limitations and Edge Cases

**1. Dramatically different content.** If clip A is a dark interior and clip B is a bright exterior, forcing them to the same color profile will make one or both look wrong. The solution: group clips by scene type and match within groups.

**2. Intentional color shifts.** Some narratives use color changes to signal time, mood, or location changes. The pipeline should allow per-segment target profiles.

**3. HDR content.** The pipeline assumes 8-bit SDR. For HDR (10-bit, PQ/HLG), the color space math changes significantly. LAB is designed for SDR viewing conditions.

**4. Temporal flickering.** If the LUT is computed from a single representative frame, it might not generalize well to all frames in a clip (especially if the clip has large brightness variations). Solution: compute the LUT from multiple representative frames and average.

Color consistency is the difference between "a collection of AI clips" and "a video." The techniques in this post --- histogram matching, Reinhard transfer, LUT generation, and CIEDE2000 verification --- form a complete toolkit. Implement the automated pipeline, integrate it into your generation workflow, and your multi-shot projects will look like they were shot by one camera with one colorist.
